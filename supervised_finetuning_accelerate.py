"""Accelerate SFTè®­ç»ƒè„šæœ¬"""  # é¡¶éƒ¨æ–‡æ¡£å­—ç¬¦ä¸²ï¼Œè¯´æ˜è¯¥æ–‡ä»¶å®ç°åŠ é€Ÿç‰ˆç›‘ç£å¾®è°ƒæµç¨‹

# -*- coding: utf-8 -*-  # æŒ‡å®šæºç ä½¿ç”¨UTF-8ç¼–ç ï¼Œä¿è¯ä¸­æ–‡æ³¨é‡Šä¸ä¼šä¹±ç 
# Copyright 2023 XuMing(xuming624@qq.com) and The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");  # ç‰ˆæƒå£°æ˜ï¼Œè¡¨æ˜ä½¿ç”¨Apache-2.0åè®®
# you may not use this file except in compliance with the License.  # å‘ŠçŸ¥ä½¿ç”¨è€…å¿…é¡»éµå®ˆè®¸å¯è¯æ¡æ¬¾
# You may obtain a copy of the License at  # æä¾›è®¸å¯è¯è·å–æ–¹å¼
#
#     http://www.apache.org/licenses/LICENSE-2.0  # Apache-2.0åè®®çš„å®˜æ–¹é“¾æ¥
#
# Unless required by applicable law or agreed to in writing, software  # åè®®æ¡æ¬¾è¯´æ˜æ— æ‹…ä¿
# distributed under the License is distributed on an "AS IS" BASIS,  # è½¯ä»¶æŒ‰â€œç°çŠ¶â€æä¾›
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  # ä¸æä¾›ä»»ä½•å½¢å¼çš„æ‹…ä¿
# See the License for the specific language governing permissions and  # å¼•å¯¼æŸ¥çœ‹å…·ä½“è®¸å¯æ¡æ¬¾
# limitations under the License.  # æé†’å­˜åœ¨çš„é™åˆ¶

import math  # å¯¼å…¥æ•°å­¦åº“ï¼Œç”¨äºè®¡ç®—å›°æƒ‘åº¦perplexityç­‰æŒ‡æ•°è¿ç®—
import os  # å¯¼å…¥æ“ä½œç³»ç»Ÿåº“ï¼Œç”¨äºç¯å¢ƒå˜é‡å’Œè·¯å¾„å¤„ç†
import sys  # å¯¼å…¥sysä»¥è¯»å–å‘½ä»¤è¡Œå‚æ•°ä»¥åŠè„šæœ¬é€€å‡º
from dataclasses import dataclass, field  # dataclassæä¾›ç±»è‡ªåŠ¨ç”Ÿæˆ__init__ç­‰æ–¹æ³•ï¼›fieldç”¨äºå­—æ®µé»˜è®¤å€¼
from glob import glob  # globç”¨äºåŒ¹é…ç›®å½•ä¸‹æ»¡è¶³æ¨¡å¼çš„æ–‡ä»¶åˆ—è¡¨
from typing import Literal, Optional, Tuple  # ç±»å‹æç¤ºå·¥å…·ï¼ŒLiteralé™å®šå­—ç¬¦ä¸²å–å€¼èŒƒå›´

import torch  # å¯¼å…¥PyTorchæ ¸å¿ƒåº“ï¼Œç”¨äºå¼ é‡è¿ç®—ä¸æ¨¡å‹è®­ç»ƒ
import torch.utils.data  # å¯¼å…¥æ•°æ®ç›¸å…³å·¥å…·ï¼Œå¦‚DataLoader
from datasets import load_dataset  # HuggingFace datasetsåŠ è½½å™¨ï¼Œç”¨äºè¯»å–æ•°æ®é›†
from loguru import logger  # å¼•å…¥loguruåº“ç”¨äºç»“æ„åŒ–æ—¥å¿—è¾“å‡º
from peft import LoraConfig, TaskType, get_peft_model, PeftModel, prepare_model_for_kbit_training  # å¼•å…¥PEFTç›¸å…³APIï¼Œç”¨äºé…ç½®LoRAåŠä½æ¯”ç‰¹è®­ç»ƒ
from transformers import (  # å¼•å…¥transformersåº“ä¸­æ¨¡å‹ä¸è®­ç»ƒæ‰€éœ€ç»„ä»¶
    AutoConfig,  # è‡ªåŠ¨åŠ è½½æ¨¡å‹é…ç½®
    AutoModelForCausalLM,  # è‡ªåŠ¨åŠ è½½å› æœè¯­è¨€æ¨¡å‹
    AutoTokenizer,  # è‡ªåŠ¨åŠ è½½åˆ†è¯å™¨
    HfArgumentParser,  # ç”¨äºè§£æå‘½ä»¤è¡Œå‚æ•°ä¸ºdataclass
    Seq2SeqTrainingArguments,  # å¤ç”¨Seq2Seqè®­ç»ƒå‚æ•°å®¹å™¨ï¼ˆå³ä½¿æ˜¯CausalLMä¹Ÿå¯ä½¿ç”¨ï¼‰
    set_seed,  # transformersæä¾›çš„éšæœºç§å­è®¾ç½®å‡½æ•°ï¼ˆè™½ç„¶æœ¬è„šæœ¬ä¸»è¦ä½¿ç”¨accelerateæä¾›çš„ç‰ˆæœ¬ï¼‰
    BitsAndBytesConfig,  # é‡åŒ–é…ç½®ç±»ï¼Œç”¨äº4bit/8bité‡åŒ–
    DataCollatorForSeq2Seq,  # æ•°æ®æ•´ç†å™¨ï¼Œè´Ÿè´£åŠ¨æ€paddingä¸labelå¯¹é½
    get_linear_schedule_with_warmup,  # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œçº¿æ€§é¢„çƒ­+çº¿æ€§ä¸‹é™
)
from transformers.trainer_pt_utils import LabelSmoother  # å¼•å…¥æ ‡ç­¾å¹³æ»‘å·¥å…·ï¼Œç”¨äºå¿½ç•¥padä½ç½®çš„loss
from tqdm.auto import tqdm  # å¼•å…¥tqdmè‡ªåŠ¨é€‰æ‹©å‰ç«¯ï¼Œæä¾›è®­ç»ƒè¿›åº¦æ¡æ˜¾ç¤º

from accelerate import Accelerator  # å¼•å…¥Acceleratorï¼Œç”¨äºå°è£…åˆ†å¸ƒå¼è®­ç»ƒæµç¨‹
from accelerate.utils import set_seed as accelerate_set_seed  # ä»accelerateä¸­å¯¼å…¥ä¸“ç”¨çš„éšæœºç§å­è®¾ç½®å‡½æ•°

is_flash_attn_2_available = False  # åˆå§‹åŒ–æ ‡å¿—ä½ï¼Œè·Ÿè¸ªæ˜¯å¦æˆåŠŸå¯¼å…¥FlashAttention-2
try:  # å°è¯•å¯¼å…¥FlashAttention-2ç›¸å…³å‡½æ•°
    from flash_attn import flash_attn_func, flash_attn_varlen_func  # å¦‚æœå¯ç”¨ï¼Œå¯¼å…¥ä¸»è¦åŠ é€Ÿå‡½æ•°
    from flash_attn.bert_padding import pad_input, unpad_input  # å¯¼å…¥paddingå·¥å…·ä»¥é€‚é…å˜é•¿è¾“å…¥

    is_flash_attn_2_available = True  # å¯¼å…¥æˆåŠŸåˆ™æ ‡è®°ä¸ºå¯ç”¨çŠ¶æ€
except ImportError:  # å¦‚æœå¯¼å…¥å¤±è´¥
    is_flash_attn_2_available = False  # ä¿æŒæ ‡å¿—ä½ä¸ºFalseï¼Œåç»­é€»è¾‘ä¼šè·³è¿‡ç›¸å…³ä¼˜åŒ–
from template import get_conv_template  # å¯¼å…¥å¯¹è¯æ¨¡æ¿æ„é€ å‡½æ•°ï¼Œç”¨äºå°†å¤šè½®å¯¹è¯è½¬æˆæ¨¡å‹è¾“å…¥


@dataclass  # ä½¿ç”¨dataclassè‡ªåŠ¨ç”Ÿæˆå‚æ•°ç±»çš„åˆå§‹åŒ–å‡½æ•°
class ModelArguments:
    """Arguments pertaining to which model/config/tokenizer we are going to fine-tune."""  # å®˜æ–¹åŸå§‹æ³¨é‡Šï¼Œæ¦‚è¿°æ¨¡å‹ç›¸å…³å‚æ•°
    model_name_or_path: Optional[str] = field(default=None)  # æŒ‡å®šåŸºç¡€æ¨¡å‹æƒé‡è·¯å¾„æˆ–huggingfaceæ¨¡å‹å
    load_in_8bit: bool = field(default=False)  # æ˜¯å¦ä»¥8bité‡åŒ–æ¨¡å¼åŠ è½½æ¨¡å‹ï¼ŒèŠ‚çœæ˜¾å­˜
    load_in_4bit: bool = field(default=False)  # æ˜¯å¦ä»¥4bité‡åŒ–æ¨¡å¼åŠ è½½æ¨¡å‹ï¼Œè¿›ä¸€æ­¥å‹ç¼©æ˜¾å­˜
    tokenizer_name_or_path: Optional[str] = field(default=None)  # å•ç‹¬æŒ‡å®šåˆ†è¯å™¨è·¯å¾„ï¼›ä¸ºç©ºåˆ™å¤ç”¨æ¨¡å‹è·¯å¾„
    cache_dir: Optional[str] = field(default=None)  # æŒ‡å®šæ¨¡å‹/æ•°æ®ç¼“å­˜ç›®å½•
    model_revision: Optional[str] = field(default="main")  # HuggingFaceæ¨¡å‹ç‰ˆæœ¬æ ‡ç­¾ï¼Œä¾‹å¦‚"main"æˆ–å…·ä½“commit
    hf_hub_token: Optional[str] = field(default=None)  # è®¿é—®ç§æœ‰æ¨¡å‹ä»“åº“æ—¶ä½¿ç”¨çš„HFä»¤ç‰Œ
    use_fast_tokenizer: bool = field(default=False)  # æ˜¯å¦ä½¿ç”¨åŸºäºtokenizersåº“çš„é«˜é€Ÿåˆ†è¯å™¨
    torch_dtype: Optional[str] = field(default="float16")  # è¯»å–æ¨¡å‹æ—¶çš„é»˜è®¤dtypeï¼Œå­—ç¬¦ä¸²åœ¨ä¸‹æ¸¸è½¬æ¢ä¸ºtorch dtype
    device_map: Optional[str] = field(default="auto")  # å½“å¯ç”¨device_mapæ—¶æŒ‡å®šæ˜ å°„ç­–ç•¥ï¼Œä¾‹å¦‚"auto"
    trust_remote_code: bool = field(default=True)  # æ˜¯å¦ä¿¡ä»»è¿œç¨‹ä»“åº“è‡ªå®šä¹‰ä»£ç ï¼ˆå¿…è¦æ—¶å¯ç”¨ï¼‰
    rope_scaling: Optional[Literal["linear", "dynamic"]] = field(default=None)  # æŒ‡å®šRoPEä½ç½®ç¼–ç ç¼©æ”¾ç­–ç•¥
    flash_attn: Optional[bool] = field(  # æ§åˆ¶æ˜¯å¦å°è¯•å¯ç”¨FlashAttention-2
        default=False,
        metadata={"help": "Enable FlashAttention-2 for faster training."}  # CLIå¸®åŠ©æ–‡æœ¬
    )


@dataclass  # ä½¿ç”¨dataclassæ‰¿è½½æ•°æ®ç›¸å…³å‚æ•°
class DataArguments:
    dataset_name: Optional[str] = field(default=None,
                                        metadata={"help": "The name of the dataset to use (via the datasets library)."})  # è‹¥æŒ‡å®šåˆ™ä»HF HubåŠ è½½æ•°æ®é›†
    dataset_config_name: Optional[str] = field(default=None, metadata={
        "help": "The configuration name of the dataset to use (via the datasets library)."})  # æŒ‡å®šæ•°æ®é›†é…ç½®å­é›†
    train_file_dir: str = field(default=None, metadata={"help": "Path to the training data."})  # æœ¬åœ°è®­ç»ƒæ•°æ®æ–‡ä»¶ç›®å½•
    validation_file_dir: str = field(default=None, metadata={"help": "Path to the validation data."})  # æœ¬åœ°éªŒè¯æ•°æ®ç›®å½•
    max_train_samples: Optional[int] = field(default=None)  # é™åˆ¶è®­ç»ƒæ ·æœ¬æ•°é‡ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨å…¨éƒ¨
    max_eval_samples: Optional[int] = field(default=None)  # é™åˆ¶éªŒè¯æ ·æœ¬æ•°é‡
    overwrite_cache: bool = field(default=False, metadata={"help": "Overwrite the cached training and evaluation sets"})  # æ˜¯å¦é‡å»ºç¼“å­˜
    validation_split_percentage: Optional[int] = field(default=1)  # å½“æ²¡æœ‰éªŒè¯é›†æ—¶ï¼Œä»è®­ç»ƒé›†ä¸­åˆ‡åˆ†ç™¾åˆ†æ¯”
    preprocessing_num_workers: Optional[int] = field(default=None)  # æŒ‡å®šmapå‡½æ•°çš„å¹¶è¡Œè¿›ç¨‹æ•°
    ignore_pad_token_for_loss: bool = field(default=True)  # æ˜¯å¦åœ¨lossè®¡ç®—æ—¶å¿½ç•¥padæ ‡è®°


@dataclass  # é¢å¤–è„šæœ¬è¡Œä¸ºå‚æ•°
class ScriptArguments:
    use_peft: bool = field(default=True)  # æ˜¯å¦å¯ç”¨PEFT (LoRA/QLoRA) å¾®è°ƒæ¨¡å¼
    train_on_inputs: bool = field(default=False)  # æ˜¯å¦è®©è¾“å…¥tokenä¹Ÿå‚ä¸lossï¼ˆä¸€èˆ¬SFTå¿½ç•¥è¾“å…¥æ¢¯åº¦ï¼‰
    target_modules: Optional[str] = field(default="all")  # LoRAæ³¨å…¥çš„ç›®æ ‡æ¨¡å—åˆ—è¡¨ï¼Œé€—å·åˆ†éš”ï¼›allè¡¨ç¤ºè‡ªåŠ¨å‘ç°
    lora_rank: Optional[int] = field(default=8)  # LoRAç§©rï¼Œæ§åˆ¶é¢å¤–çŸ©é˜µçš„ç§©
    lora_dropout: Optional[float] = field(default=0.05)  # LoRA Dropoutæ¯”ä¾‹ï¼Œé˜²è¿‡æ‹Ÿåˆ
    lora_alpha: Optional[float] = field(default=32.0)  # LoRAç¼©æ”¾å› å­alpha
    modules_to_save: Optional[str] = field(default=None)  # æŒ‡å®šé™¤LoRAå¤–éœ€è¦ä¿å­˜çš„æ¨¡å—ï¼Œä¾‹å¦‚embed_tokens
    peft_path: Optional[str] = field(default=None)  # è‹¥æä¾›å·²æœ‰LoRAæƒé‡è·¯å¾„ï¼Œåˆ™åŸºäºå…¶ç»§ç»­è®­ç»ƒ
    qlora: bool = field(default=False)  # æ˜¯å¦ä½¿ç”¨QLoRAï¼ˆé…åˆ4bité‡åŒ–ï¼‰
    model_max_length: int = field(default=2048)  # å•æ¡æ ·æœ¬æœ€å¤§tokené•¿åº¦ï¼Œç”¨äºæˆªæ–­
    template_name: Optional[str] = field(default="vicuna")  # é€‰æ‹©å¯¹è¯æ¨¡æ¿åç§°ï¼Œç¡®ä¿è®­ç»ƒæ¨ç†ä¸€è‡´
    use_tensor_parallel: bool = field(  # æ˜¯å¦å¯ç”¨å¼ é‡å¹¶è¡Œï¼ˆæ¨¡å‹åˆ†ç‰‡åˆ°å¤šä¸ªGPUï¼‰
        default=False,
        metadata={"help": "Whether to use tensor parallelism for large models"}  # CLIå¸®åŠ©æ–‡æœ¬
    )


def find_all_linear_names(model, int4=False, int8=False):
    """æŸ¥æ‰¾æ¨¡å‹ä¸­æ‰€æœ‰çš„çº¿æ€§å±‚åç§°"""  # è¿”å›æ¨¡å‹å†…å¯æ³¨å…¥LoRAçš„çº¿æ€§å±‚åç§°é›†åˆ
    cls = torch.nn.Linear  # é»˜è®¤åŒ¹é…PyTorchæ ‡å‡†Linearå±‚
    if int4 or int8:  # å¦‚æœä½¿ç”¨äº†ä½æ¯”ç‰¹é‡åŒ–
        import bitsandbytes as bnb  # å»¶è¿Ÿå¯¼å…¥bitsandbytesï¼Œé¿å…éé‡åŒ–æƒ…å†µä¸‹çš„ä¾èµ–
        if int4:  # 4bité‡åŒ–æ—¶
            cls = bnb.nn.Linear4bit  # åˆ‡æ¢åŒ¹é…ç±»åˆ°4bitçº¿æ€§å±‚
        elif int8:  # 8bité‡åŒ–æ—¶
            cls = bnb.nn.Linear8bit  # åˆ‡æ¢åŒ¹é…ç±»åˆ°8bitçº¿æ€§å±‚
    lora_module_names = set()  # åˆå§‹åŒ–é›†åˆå­˜å‚¨ç¬¦åˆæ¡ä»¶çš„æ¨¡å—åç§°
    for name, module in model.named_modules():  # éå†æ¨¡å‹ä¸­çš„å…¨éƒ¨å­æ¨¡å—ï¼Œnameå½¢å¦‚"model.layers.0.self_attn.q_proj"
        if isinstance(module, cls):  # ä»…å¤„ç†ä¸ç›®æ ‡Linearç±»åŒ¹é…çš„æ¨¡å—
            if 'lm_head' in name:  # è·³è¿‡è¾“å‡ºå±‚lm_headï¼Œé¿å…åœ¨æœ€ç»ˆæŠ•å½±å±‚æ³¨å…¥LoRA
                continue
            if 'output_layer' in name:  # æŸäº›æ¨¡å‹è‡ªå®šä¹‰çš„è¾“å‡ºå±‚åŒæ ·è·³è¿‡
                continue
            names = name.split('.')  # å°†å±‚åç§°æŒ‰"."æ‹†åˆ†ï¼Œä¾‹å¦‚['model','layers','0','self_attn','q_proj']
            lora_module_names.add(names[0] if len(names) == 1 else names[-1])  # å–æœ€åä¸€çº§æ¨¡å—åç§°ç™»è®°é›†åˆ
    return sorted(lora_module_names)  # è¿”å›æ’åºåçš„æ¨¡å—åˆ—è¡¨ï¼Œæ–¹ä¾¿å¯é‡å¤ä½¿ç”¨


def save_model(model, tokenizer, output_dir):
    """Save the model and the tokenizer."""  # ä¿å­˜æ¨¡å‹ä¸åˆ†è¯å™¨åˆ°æŒ‡å®šç›®å½•
    os.makedirs(output_dir, exist_ok=True)  # è‹¥è¾“å‡ºç›®å½•ä¸å­˜åœ¨åˆ™åˆ›å»ºï¼Œexist_oké¿å…é‡å¤æŠ¥é”™

    model_to_save = model.module if hasattr(model, "module") else model  # è‹¥æ¨¡å‹è¢«DDPåŒ…è£¹ï¼Œå–å…¶å†…éƒ¨åŸå§‹æ¨¡å‹
    model_to_save.save_pretrained(output_dir)  # ä»¥HuggingFaceæ ¼å¼ä¿å­˜æƒé‡å’Œé…ç½®
    tokenizer.save_pretrained(output_dir)  # ä¿å­˜åˆ†è¯å™¨è¯è¡¨åŠç›¸å…³é…ç½®


def print_trainable_parameters(model):
    """Prints the number of trainable parameters in the model."""  # è¾“å‡ºæ¨¡å‹å¯è®­ç»ƒå‚æ•°å æ¯”
    trainable_params = 0  # åˆå§‹åŒ–å¯è®­ç»ƒå‚æ•°è®¡æ•°
    all_param = 0  # åˆå§‹åŒ–å…¨éƒ¨å‚æ•°è®¡æ•°
    for _, param in model.named_parameters():  # éå†æ‰€æœ‰å‚æ•°å¼ é‡
        all_param += param.numel()  # numel()è¿”å›å¼ é‡å…ƒç´ ä¸ªæ•°ï¼Œç´¯åŠ å¾—åˆ°æ€»å‚æ•°é‡
        if param.requires_grad:  # ä»…ç»Ÿè®¡éœ€è¦æ¢¯åº¦çš„å‚æ•°
            trainable_params += param.numel()
    print(  # é€šè¿‡æ ‡å‡†è¾“å‡ºæ‰“å°ç»“æœï¼ŒåŒ…å«æ€»é‡ã€å¯è®­ç»ƒé‡ã€ç™¾åˆ†æ¯”
        f"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}"
    )


def load_datasets(data_args, model_args):
    """Load datasets from files or HuggingFace hub"""  # æ ¹æ®ä¼ å‚å†³å®šæ•°æ®é›†æ¥æº
    if data_args.dataset_name is not None:  # è‹¥æä¾›æ•°æ®é›†åç§°ï¼Œä»HF Hubä¸‹è½½
        raw_datasets = load_dataset(  # load_datasetä¼šè¿”å›DatasetDictï¼ŒåŒ…å«train/validationç­‰split
            data_args.dataset_name,
            data_args.dataset_config_name,
            cache_dir=model_args.cache_dir,
        )
        if "validation" not in raw_datasets.keys():  # è‹¥ç¼ºå°‘validationåˆ’åˆ†
            shuffled_train_dataset = raw_datasets["train"].shuffle(seed=42)  # å…ˆå¯¹trainéšæœºæ‰“ä¹±ï¼Œä¿è¯åˆ’åˆ†å‡åŒ€
            split = shuffled_train_dataset.train_test_split(  # åˆ©ç”¨datasetså†…ç½®å‡½æ•°åˆ‡åˆ†
                test_size=data_args.validation_split_percentage / 100,
                seed=42
            )
            raw_datasets["train"] = split["train"]  # å°†åˆ‡åˆ†åçš„è®­ç»ƒéƒ¨åˆ†å†™å›train
            raw_datasets["validation"] = split["test"]  # åˆ‡åˆ†å‡ºçš„æµ‹è¯•éƒ¨åˆ†ä½œä¸ºvalidation
    else:  # å¦åˆ™ä»æœ¬åœ°JSON/JSONLæ–‡ä»¶åŠ è½½
        data_files = {}  # åˆå§‹åŒ–æ–‡ä»¶è·¯å¾„å­—å…¸
        if data_args.train_file_dir is not None and os.path.exists(data_args.train_file_dir):  # åˆ¤æ–­è®­ç»ƒæ•°æ®ç›®å½•æ˜¯å¦å­˜åœ¨
            train_data_files = glob(f'{data_args.train_file_dir}/**/*.json', recursive=True) + glob(
                f'{data_args.train_file_dir}/**/*.jsonl', recursive=True)  # é€’å½’åŒ¹é…æ‰€æœ‰json/jsonlæ–‡ä»¶
            logger.info(f"train files: {train_data_files}")  # æ‰“å°åŒ¹é…æ–‡ä»¶åˆ—è¡¨
            data_files["train"] = train_data_files  # åŠ å…¥trainé”®
        if data_args.validation_file_dir is not None and os.path.exists(data_args.validation_file_dir):  # åŒç†éªŒè¯é›†
            eval_data_files = glob(f'{data_args.validation_file_dir}/**/*.json', recursive=True) + glob(
                f'{data_args.validation_file_dir}/**/*.jsonl', recursive=True)
            logger.info(f"eval files: {eval_data_files}")
            data_files["validation"] = eval_data_files
        raw_datasets = load_dataset(  # ä»¥jsonåŠ è½½å™¨è¯»å–æœ¬åœ°å¤šä¸ªæ–‡ä»¶åˆå¹¶ä¸ºå•ä¸ªDataset
            'json',
            data_files=data_files,
            cache_dir=model_args.cache_dir,
        )
        if "validation" not in raw_datasets.keys():  # è‹¥ä»ç„¶æ²¡æœ‰validation split
            shuffled_train_dataset = raw_datasets["train"].shuffle(seed=42)  # å…ˆéšæœºæ‰“ä¹±è®­ç»ƒé›†
            split = shuffled_train_dataset.train_test_split(  # ä¾ç…§æ¯”ä¾‹åˆ‡åˆ†
                test_size=float(data_args.validation_split_percentage / 100),
                seed=42
            )
            raw_datasets["train"] = split["train"]  # æ›´æ–°trainé›†åˆ
            raw_datasets["validation"] = split["test"]  # æ›´æ–°validationé›†åˆ

    logger.info(f"Raw datasets: {raw_datasets}")  # è®°å½•æœ€ç»ˆæ•°æ®é›†æ¦‚å†µ
    return raw_datasets  # è¿”å›DatasetDictä¾›åç»­ä½¿ç”¨


def create_preprocess_function(tokenizer, prompt_template, script_args, IGNORE_INDEX):
    """Create preprocessing function for datasets"""  # è¿”å›é—­åŒ…ï¼Œç”¨äºå°†åŸå§‹å¯¹è¯æ ·æœ¬è½¬ä¸ºæ¨¡å‹è¾“å…¥
    max_length = script_args.model_max_length  # ä»è„šæœ¬å‚æ•°è¯»å–å…¨å±€æœ€å¤§tokené•¿åº¦ï¼Œç”¨äºåç»­æˆªæ–­

    def preprocess_function(examples):
        """
        Preprocessing the datasets.
            part of code modified from https://github.com/lm-sys/FastChat
        """  # å†…éƒ¨å‡½æ•°æŒ‰datasets.mapæ¥å£å¤„ç†ä¸€ä¸ªæ‰¹æ¬¡examples
        input_ids_list = []  # ä¿å­˜æ¯æ¡æ ·æœ¬çš„tokenåºåˆ—ï¼Œå½¢çŠ¶ä¸º[List[int]], æ¯æ¡é•¿åº¦<=max_length
        attention_mask_list = []  # ä¿å­˜attention_maskï¼Œä¸input_idsç­‰é•¿ï¼Œ1è¡¨ç¤ºæœ‰æ•ˆtoken
        targets_list = []  # ä¿å­˜labelsï¼Œå¿½ç•¥è¾“å…¥éƒ¨åˆ†ç”¨IGNORE_INDEXå ä½
        roles = ["human", "gpt"]  # é¢„æœŸè§’è‰²é¡ºåºï¼Œäººç±»æé—®->æ¨¡å‹å›ç­”

        def get_dialog(examples):  # å†…éƒ¨ç”Ÿæˆå™¨ï¼Œå°†åŸå§‹conversationç»“æ„å±•å¼€æˆå­—ç¬¦ä¸²åºåˆ—
            system_prompts = examples.get("system_prompt", "")  # å°è¯•è¯»å–æ‰¹æ¬¡ä¸­çš„ç³»ç»Ÿæç¤ºåˆ—è¡¨ï¼Œå¯èƒ½ä¸ºç©º
            for i, source in enumerate(examples['conversations']):  # éå†æ¯æ¡æ ·æœ¬çš„conversationå­—æ®µ
                system_prompt = ""  # é»˜è®¤ç³»ç»Ÿæç¤ºä¸ºç©º
                if len(source) < 2:  # å¯¹è¯è½®æ¬¡ä¸è¶³2ï¼ˆç¼ºå°‘é—®ç­”å¯¹ï¼‰åˆ™è·³è¿‡
                    continue
                data_role = source[0].get("from", "")  # è¯»å–ç¬¬ä¸€æ¡æ¶ˆæ¯çš„è§’è‰²
                if data_role == "system":  # ç¬¬ä¸€æ¡å¦‚æœæ˜¯ç³»ç»Ÿæ¶ˆæ¯
                    system_prompt = source[0]["value"]  # ä¿å­˜ç³»ç»Ÿæç¤ºæ–‡æœ¬
                    source = source[1:]  # å°†åç»­å¯¹è¯ä½œä¸ºçœŸæ­£çš„äº¤äº’
                    data_role = source[0].get("from", "")  # æ›´æ–°ç¬¬ä¸€æ¡æ¶ˆæ¯çš„è§’è‰²
                if data_role not in roles or data_role != roles[0]:  # å¦‚æœç¬¬ä¸€æ¡ä¸æ˜¯humanè§’è‰²
                    source = source[1:]  # è·³è¿‡è¿™ä¸€æ¡ï¼Œå°è¯•ä»ä¸‹ä¸€æ¡å¼€å§‹
                if len(source) < 2:  # å†æ¬¡æ£€æŸ¥å‰©ä½™é•¿åº¦ï¼Œç¡®ä¿è‡³å°‘ä¸€å¯¹é—®ç­”
                    continue
                messages = []  # å­˜å‚¨æŒ‰é¡ºåºæ’åˆ—çš„æ¶ˆæ¯å†…å®¹
                for j, sentence in enumerate(source):  # éå†å‰©ä½™çš„æ¶ˆæ¯
                    data_role = sentence.get("from", "")  # å½“å‰æ¶ˆæ¯çš„è§’è‰²
                    if data_role not in roles:  # å‘ç°æœªçŸ¥è§’è‰²
                        logger.warning(f"unknown role: {data_role}, {i}. (ignored)")  # è®°å½•è­¦å‘Šå¹¶å¿½ç•¥æ•´æ¡æ ·æœ¬
                        break
                    if data_role == roles[j % 2]:  # éªŒè¯è§’è‰²é¡ºåºæ˜¯å¦ä¸human/gptäº¤æ›¿åŒ¹é…
                        messages.append(sentence["value"])  # æ”¶é›†æ¶ˆæ¯æ–‡æœ¬
                if len(messages) % 2 != 0:  # è‹¥æ¶ˆæ¯æ•°é‡ä¸æ˜¯å¶æ•°ï¼ˆé—®ç­”ä¸æˆå¯¹ï¼‰åˆ™è·³è¿‡
                    continue
                history_messages = [[messages[k], messages[k + 1]] for k in range(0, len(messages), 2)]  # å°†åˆ—è¡¨æ‹†æˆ[[é—®,ç­”], ...]
                if not system_prompt:  # å¦‚æœå½“å‰æ ·æœ¬æ²¡å†™ç³»ç»Ÿæç¤º
                    system_prompt = system_prompts[i] if system_prompts else ""  # å°è¯•ä»æ‰¹æ¬¡å­—æ®µè¡¥é½
                yield prompt_template.get_dialog(history_messages, system_prompt=system_prompt)  # ä½¿ç”¨æ¨¡æ¿è½¬æ¢ä¸ºæ‰å¹³åŒ–å­—ç¬¦ä¸²åºåˆ—

        for dialog in get_dialog(examples):  # éå†ç”Ÿæˆçš„æ¨¡æ¿åŒ–dialogï¼Œdialogä¸ºé•¿åº¦2nçš„åˆ—è¡¨[é¦–è½®prompt,é¦–è½®ç­”å¤,...]
            input_ids, labels = [], []  # åˆå§‹åŒ–è¯¥æ ·æœ¬çš„è¾“å…¥åºåˆ—ä¸æ ‡ç­¾åºåˆ—

            for i in range(len(dialog) // 2):  # æ¯æ¬¡å¤„ç†ä¸€å¯¹é—®ç­”ï¼ˆç´¢å¼•iå¯¹åº”ç¬¬iè½®å¯¹è¯ï¼‰
                source_ids = tokenizer.encode(text=dialog[2 * i], add_special_tokens=(i == 0))  # å°†æé—®ç¼–ç ä¸ºtokenåˆ—è¡¨ï¼Œé¦–è½®å…è®¸åŠ å…¥BOSç­‰ç‰¹æ®Šç¬¦å·
                target_ids = tokenizer.encode(text=dialog[2 * i + 1], add_special_tokens=False)  # å°†å›ç­”ç¼–ç ä¸ºtokenåˆ—è¡¨ï¼Œä¸æ·»åŠ é¢å¤–ç¬¦å·

                total_len = len(source_ids) + len(target_ids)  # å½“å‰è½®æ¬¡é—®ç­”çš„æ€»tokenæ•°ï¼Œç”¨äºæŒ‰æ¯”ä¾‹åˆ†é…æˆªæ–­é•¿åº¦
                max_source_len = int(max_length * (len(source_ids) / total_len))  # è®¡ç®—sourceå¯å ç”¨é•¿åº¦ï¼Œå½¢å¦‚floor(max_len * source_ratio)
                max_target_len = int(max_length * (len(target_ids) / total_len))  # åŒç†ï¼Œè®¡ç®—targetå…è®¸é•¿åº¦

                if len(source_ids) > max_source_len:  # è‹¥æé—®è¶…è¿‡åˆ†é…çš„é•¿åº¦
                    source_ids = source_ids[:max_source_len]  # æˆªæ–­source_idsï¼Œå½¢çŠ¶ä¿æŒ1D
                if len(target_ids) > max_target_len - 1:  # é¢„ç•™1ä¸ªä½ç½®ç»™EOS token
                    target_ids = target_ids[:max_target_len - 1]  # æˆªæ–­å›ç­”token
                if len(source_ids) > 0 and source_ids[0] == tokenizer.eos_token_id:  # å¤„ç†ç¼–ç æ—¶å¯èƒ½å¼€å¤´æ˜¯EOSçš„æƒ…å†µ
                    source_ids = source_ids[1:]
                if len(target_ids) > 0 and target_ids[-1] == tokenizer.eos_token_id:  # è‹¥å›ç­”æœ«å°¾å·²æœ‰EOSï¼Œåˆ™ç§»é™¤é¿å…é‡å¤
                    target_ids = target_ids[:-1]
                if len(input_ids) + len(source_ids) + len(target_ids) + 1 > max_length:  # æ£€æŸ¥ç´¯è®¡é•¿åº¦+1ï¼ˆè¿½åŠ EOSï¼‰æ˜¯å¦è¶…é™
                    break  # è¶…è¿‡ä¸Šé™åˆ™åœæ­¢æ·»åŠ åç»­è½®æ¬¡

                input_ids += source_ids + target_ids + [tokenizer.eos_token_id]  # å°†é—®ç­”åŠç»“å°¾EOSæ‹¼æ¥è¿›è¾“å…¥ï¼Œåºåˆ—é•¿åº¦æ›´æ–°ä¸ºæ—§å€¼+len(src)+len(tgt)+1
                if script_args.train_on_inputs:  # è‹¥è®­ç»ƒæ—¶åŒ…å«è¾“å…¥tokensåœ¨lossä¸­
                    labels += source_ids + target_ids + [tokenizer.eos_token_id]  # labelsä¸input_idså®Œå…¨å¯¹é½
                else:
                    labels += [IGNORE_INDEX] * len(source_ids) + target_ids + [tokenizer.eos_token_id]  # è¾“å…¥éƒ¨åˆ†ç”¨IGNORE_INDEXå¡«å……ï¼Œå¿½ç•¥æ¢¯åº¦

            input_ids_list.append(input_ids)  # è¿½åŠ å½“å‰æ ·æœ¬çš„tokenåºåˆ—ï¼Œå½¢çŠ¶: [seq_len]
            attention_mask_list.append([1] * len(input_ids))  # attention_maskä¸input_idsç­‰é•¿ï¼Œå…¨éƒ¨ä¸º1ï¼ˆåç»­collatorä¼špadï¼‰
            targets_list.append(labels)  # è¿½åŠ æ ‡ç­¾åºåˆ—ï¼Œé•¿åº¦ä¸input_idsä¸€è‡´

        return dict(  # è¿”å›datasetsè¦æ±‚çš„å­—å…¸ï¼Œé”®ä¸ºå­—æ®µåï¼Œå€¼ä¸ºæ‰¹é‡æ•°æ®åˆ—è¡¨
            input_ids=input_ids_list,
            attention_mask=attention_mask_list,
            labels=targets_list,
        )

    return preprocess_function  # è¿”å›é—­åŒ…ä¾›mapè°ƒç”¨


def filter_empty_labels(example, IGNORE_INDEX):
    """Remove empty labels dataset."""  # è¿‡æ»¤æ‰labelså…¨ä¸ºIGNORE_INDEXçš„æ ·æœ¬ï¼Œé¿å…æ— ç›‘ç£ä¿¡å·
    return not all(label == IGNORE_INDEX for label in example["labels"])  # å¦‚æœå­˜åœ¨è‡³å°‘ä¸€ä¸ªçœŸå®æ ‡ç­¾åˆ™ä¿ç•™


def check_and_optimize_memory():
    """æ£€æŸ¥å¹¶ä¼˜åŒ–GPUå†…å­˜ä½¿ç”¨"""  # æ‰“å°å½“å‰GPUå†…å­˜ä½¿ç”¨æƒ…å†µå¹¶å¯ç”¨å¯ç”¨çš„é«˜æ•ˆæ³¨æ„åŠ›å®ç°
    if not torch.cuda.is_available():  # è‹¥æ— GPUåˆ™æ— éœ€å¤„ç†
        return

    logger.info("ğŸ” æ£€æŸ¥GPUå†…å­˜çŠ¶æ€...")  # æç¤ºå¼€å§‹æ£€æŸ¥

    torch.cuda.empty_cache()  # æ¸…ç©ºPyTorchç¼“å­˜ï¼Œé‡Šæ”¾æœªä½¿ç”¨æ˜¾å­˜

    num_gpus = torch.cuda.device_count()  # è·å–GPUæ•°é‡
    for i in range(num_gpus):  # éå†æ¯å¼ GPU
        props = torch.cuda.get_device_properties(i)  # è·å–ç¡¬ä»¶å±æ€§
        total_memory = props.total_memory / 1024 ** 3  # è®¡ç®—æ€»æ˜¾å­˜ï¼Œå•ä½GB
        allocated = torch.cuda.memory_allocated(i) / 1024 ** 3  # å½“å‰å·²åˆ†é…æ˜¾å­˜GB
        cached = torch.cuda.memory_reserved(i) / 1024 ** 3  # PyTorchç¼“å­˜æ˜¾å­˜GB
        free = total_memory - allocated - cached  # ä¼°ç®—å¯ç”¨æ˜¾å­˜GB

        logger.info(f"GPU {i} ({props.name}):")  # è¾“å‡ºGPUåç§°
        logger.info(f"  æ€»å†…å­˜: {total_memory:.1f}GB")
        logger.info(f"  å·²åˆ†é…: {allocated:.1f}GB")
        logger.info(f"  å·²ç¼“å­˜: {cached:.1f}GB")
        logger.info(f"  å¯ç”¨: {free:.1f}GB")

        if free < 2.0:  # å¦‚æœå¯ç”¨æ˜¾å­˜ä½äº2GBï¼Œæç¤ºä¼˜åŒ–ç­–ç•¥
            logger.warning(f"âš ï¸ GPU {i} å¯ç”¨å†…å­˜ä¸è¶³ ({free:.1f}GB)ï¼Œå»ºè®®:")
            logger.warning("  1. ä½¿ç”¨ --load_in_4bit å¯ç”¨4bité‡åŒ–")
            logger.warning("  2. å‡å° --per_device_train_batch_size")
            logger.warning("  3. å¢åŠ  --gradient_accumulation_steps")
            logger.warning("  4. å‡å° --model_max_length")

    if hasattr(torch.backends.cuda, 'enable_flash_sdp'):  # æ£€æŸ¥æ˜¯å¦æ”¯æŒFlash SDPå®ç°
        torch.backends.cuda.enable_flash_sdp(True)  # å¯ç”¨FlashAttentionå†…æ ¸
        logger.info("âœ… å¯ç”¨Flash Attentionä¼˜åŒ–")

    if hasattr(torch.backends.cuda, 'enable_mem_efficient_sdp'):  # æ£€æŸ¥æ˜¯å¦æ”¯æŒå†…å­˜é«˜æ•ˆæ³¨æ„åŠ›
        torch.backends.cuda.enable_mem_efficient_sdp(True)
        logger.info("âœ… å¯ç”¨å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›æœºåˆ¶")


def get_unwrapped_model(model):
    """è·å–æœªåŒ…è£…çš„åŸå§‹æ¨¡å‹ï¼Œæ— è®ºå®ƒæ˜¯å¦è¢«DDPåŒ…è£…"""  # ç»Ÿä¸€è¿”å›åŸºç¡€æ¨¡å‹å¯¹è±¡
    if hasattr(model, "module"):  # å½“æ¨¡å‹è¢«DDP/DataParallelåŒ…è£…æ—¶
        return model.module  # å–å†…éƒ¨module
    return model  # å¦åˆ™ç›´æ¥è¿”å›


def main():
    os.environ.setdefault("PYTORCH_CUDA_ALLOC_CONF", "expandable_segments:True")  # è®¾ç½®CUDAæ˜¾å­˜åˆ†é…ç­–ç•¥ï¼Œå…è®¸å¼¹æ€§åˆ†æ®µåˆ†é…
    parser = HfArgumentParser((ModelArguments, DataArguments, Seq2SeqTrainingArguments, ScriptArguments))  # å»ºç«‹å‚æ•°è§£æå™¨ï¼Œè‡ªåŠ¨æ˜ å°„åˆ°å››ä¸ªdataclass

    if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):  # è‹¥å‘½ä»¤è¡Œåªç»™ä¸€ä¸ªjsonæ–‡ä»¶ï¼Œåˆ™æŒ‰jsoné…ç½®è§£åŒ…
        model_args, data_args, training_args, script_args = parser.parse_json_file(
            json_file=os.path.abspath(sys.argv[1]))  # å°†jsonè·¯å¾„è½¬ç»å¯¹è·¯å¾„åè§£æä¸ºå››ä¸ªå‚æ•°å¯¹è±¡
    else:  # å¦åˆ™ç›´æ¥ä»å‘½ä»¤è¡Œè§£æ
        model_args, data_args, training_args, script_args = parser.parse_args_into_dataclasses(look_for_args_file=False)  # parse_args_into_dataclassesè¿”å›å¯¹åº”å‚æ•°å®ä¾‹

    logger.info(f"ğŸš€ ä½¿ç”¨Accelerateåº“è¿›è¡Œå¤šGPUè®­ç»ƒ")  # æ—¥å¿—è®°å½•è®­ç»ƒæ¨¡å¼
    logger.info("ğŸš€ å¼€å§‹åˆå§‹åŒ–Accelerator...")  # æç¤ºAcceleratoråˆå§‹åŒ–
    accelerator = Accelerator()  # åˆ›å»ºAcceleratorå®ä¾‹ï¼Œå†…éƒ¨å¤„ç†åˆ†å¸ƒå¼/æ··åˆç²¾åº¦ç­‰é…ç½®
    logger.info("âœ… Acceleratoråˆå§‹åŒ–å®Œæˆ")  # æ ‡è®°åˆå§‹åŒ–å®Œæˆ
    try:  # å°è¯•æ‰“å°æ›´å¤šçŠ¶æ€ä¿¡æ¯
        logger.info(f"è®¾å¤‡: {accelerator.device}")  # è¾“å‡ºå½“å‰è®¾å¤‡
        logger.info(f"æ£€æµ‹åˆ° {accelerator.num_processes} ä¸ªè¿›ç¨‹")  # è¾“å‡ºåˆ†å¸ƒå¼è¿›ç¨‹æ•°
        logger.info(f"å½“å‰è¿›ç¨‹: {accelerator.process_index}")  # è¾“å‡ºå½“å‰rank
        logger.info(f"åˆ†å¸ƒå¼ç±»å‹: {accelerator.distributed_type}")  # è¾“å‡ºåˆ†å¸ƒå¼åç«¯ä¿¡æ¯
    except Exception:  # æ•è·å¯èƒ½çš„å±æ€§è®¿é—®å¼‚å¸¸
        logger.warning("æ— æ³•è·å–å®Œæ•´çš„Acceleratorä¿¡æ¯ï¼Œä½†è¿™ä¸å½±å“è®­ç»ƒ")  # æ‰“å°è­¦å‘Šä½†ä¸ä¸­æ–­

    logger.info(f"Model args: {model_args}")  # è®°å½•æ¨¡å‹ç›¸å…³å‚æ•°
    logger.info(f"Training args: {training_args}")  # è®°å½•è®­ç»ƒå‚æ•°
    logger.info(f"Script args: {script_args}")  # è®°å½•è„šæœ¬è‡ªå®šä¹‰å‚æ•°

    accelerate_set_seed(training_args.seed)  # è®¾ç½®å…¨å±€éšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯å¤ç°

    tokenizer_kwargs = {  # æ„é€ åŠ è½½åˆ†è¯å™¨çš„å…³é”®å­—å‚æ•°
        "cache_dir": model_args.cache_dir,
        "use_fast": model_args.use_fast_tokenizer,
        "trust_remote_code": model_args.trust_remote_code,
    }
    tokenizer_name_or_path = model_args.tokenizer_name_or_path or model_args.model_name_or_path  # ä¼˜å…ˆä½¿ç”¨å•ç‹¬æŒ‡å®šçš„åˆ†è¯å™¨è·¯å¾„ï¼Œå¦åˆ™å¤ç”¨æ¨¡å‹è·¯å¾„
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path, **tokenizer_kwargs)  # åŠ è½½åˆ†è¯å™¨å¯¹è±¡

    prompt_template = get_conv_template(script_args.template_name)  # è·å–æŒ‡å®šåç§°çš„å¯¹è¯æ¨¡æ¿ï¼Œç”¨äºè¡¥å……ç‰¹æ®Štokenå¹¶æ ¼å¼åŒ–è¾“å…¥
    if tokenizer.eos_token_id is None:  # è‹¥åˆ†è¯å™¨å°šæœªå®šä¹‰EOS
        tokenizer.eos_token = prompt_template.stop_str  # ä½¿ç”¨æ¨¡æ¿æä¾›çš„åœç”¨ç¬¦ä½œä¸ºEOS
        tokenizer.add_special_tokens({"eos_token": tokenizer.eos_token})  # æ³¨å†ŒEOS token
        logger.info(f"Add eos_token: {tokenizer.eos_token}")  # è®°å½•æ–°å¢token

    if tokenizer.bos_token_id is None:  # è‹¥ç¼ºå°‘BOS
        tokenizer.add_special_tokens({"bos_token": tokenizer.eos_token})  # å°†EOSå¤ç”¨ä¸ºBOSï¼ˆä¸å°‘å¼€æºæ¨¡å‹è¿™æ ·å¤„ç†ï¼‰
        tokenizer.bos_token_id = tokenizer.eos_token_id  # åŒæ­¥BOS ID
        logger.info(f"Add bos_token: {tokenizer.bos_token}")

    if tokenizer.pad_token_id is None:  # è‹¥ç¼ºå°‘PAD
        if tokenizer.unk_token_id is not None:  # ä¼˜å…ˆä½¿ç”¨UNKä½œä¸ºPAD
            tokenizer.pad_token = tokenizer.unk_token
        else:
            tokenizer.pad_token = tokenizer.eos_token  # å¦åˆ™é€€åŒ–ä¸ºä½¿ç”¨EOSä½œä¸ºPAD
        logger.info(f"Add pad_token: {tokenizer.pad_token}")

    IGNORE_INDEX = LabelSmoother.ignore_index if data_args.ignore_pad_token_for_loss else tokenizer.pad_token_id  # æ ¹æ®é…ç½®ç¡®å®šlossä¸­å¿½ç•¥çš„æ ‡ç­¾å€¼

    logger.info("âœ… Tokenizeré…ç½®å®Œæˆ")  # æ ‡è®°åˆ†è¯å™¨å‡†å¤‡å®Œæ¯•

    check_and_optimize_memory()  # åœ¨åŠ è½½æ¨¡å‹å‰å…ˆé‡Šæ”¾æ˜¾å­˜å¹¶å¯ç”¨å¯é€‰ä¼˜åŒ–

    logger.info("ğŸ”„ å¼€å§‹åŠ è½½æ¨¡å‹...")  # æç¤ºæ¨¡å‹åŠ è½½é˜¶æ®µ

    torch_dtype = model_args.torch_dtype  # è®°å½•æœŸæœ›çš„æ¨¡å‹dtypeï¼ˆå­—ç¬¦ä¸²å½¢å¼ï¼Œä¾‹å¦‚"float16"ï¼‰
    quantization_config = None  # é»˜è®¤ä¸ä½¿ç”¨é‡åŒ–é…ç½®
    if model_args.load_in_4bit:  # å¦‚éœ€4bité‡åŒ–
        quantization_config = BitsAndBytesConfig(  # åˆ›å»º4bité‡åŒ–é…ç½®
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch_dtype,  # è®¡ç®—æ—¶ä½¿ç”¨çš„dtypeï¼Œå¦‚float16/bfloat16
            bnb_4bit_use_double_quant=True,  # å¯ç”¨double quantå‡å°é‡åŒ–è¯¯å·®
            bnb_4bit_quant_type="nf4"  # æŒ‡å®šNF4é‡åŒ–ç±»å‹
        )
    elif model_args.load_in_8bit:  # å¦‚éœ€8bité‡åŒ–
        quantization_config = BitsAndBytesConfig(load_in_8bit=True)  # åˆ›å»º8bité‡åŒ–é…ç½®

    config_kwargs = {  # å‡†å¤‡åŠ è½½æ¨¡å‹é…ç½®çš„å‚æ•°
        "trust_remote_code": model_args.trust_remote_code,
        "cache_dir": model_args.cache_dir,
        "revision": model_args.model_revision,
        "hf_hub_token": model_args.hf_hub_token,
    }
    if model_args.flash_attn:  # å¦‚æœç”¨æˆ·è¦æ±‚å¯ç”¨FlashAttention
        if is_flash_attn_2_available:  # æ£€æŸ¥æ˜¯å¦å·²æˆåŠŸå¯¼å…¥
            config_kwargs["use_flash_attention_2"] = True  # åœ¨é…ç½®ä¸­æ‰“å¼€FlashAttentionæ ‡å¿—
            logger.info("Using FlashAttention-2 for faster training and inference.")  # è®°å½•å¯ç”¨ä¿¡æ¯
        else:
            logger.warning("FlashAttention-2 is not installed.")  # æé†’ç¼ºå°‘ä¾èµ–
    config = AutoConfig.from_pretrained(model_args.model_name_or_path, **config_kwargs)  # åŠ è½½æ¨¡å‹é…ç½®å¯¹è±¡

    total_memory = 0  # åˆå§‹åŒ–GPUæ€»æ˜¾å­˜ç»Ÿè®¡
    if torch.cuda.is_available():  # ä»…åœ¨GPUç¯å¢ƒä¸‹æ‰§è¡Œ
        num_gpus = torch.cuda.device_count()  # è·å–GPUæ•°é‡
        logger.info(f"æ£€æµ‹åˆ° {num_gpus} ä¸ªGPU")  # æ‰“å°GPUæ•°é‡

        for i in range(num_gpus):  # éå†æ¯å—GPU
            gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024 ** 3  # å•å¡æ€»æ˜¾å­˜GB
            allocated = torch.cuda.memory_allocated(i) / 1024 ** 3  # è¯¥å¡å·²åˆ†é…æ˜¾å­˜GB
            cached = torch.cuda.memory_reserved(i) / 1024 ** 3  # è¯¥å¡ç¼“å­˜æ˜¾å­˜GB
            free = gpu_memory - allocated  # ä¼°ç®—æœªåˆ†é…æ˜¾å­˜ï¼ˆå¿½ç•¥ç¼“å­˜å½±å“ï¼‰
            total_memory += gpu_memory  # ç´¯åŠ æ€»æ˜¾å­˜
            logger.info(  # è¾“å‡ºæ¯å¡è¯¦ç»†ä¿¡æ¯
                f"GPU {i}: æ€»å†…å­˜={gpu_memory:.1f}GB, å·²åˆ†é…={allocated:.1f}GB, ç¼“å­˜={cached:.1f}GB, å¯ç”¨={free:.1f}GB")

        logger.info(f"æ€»GPUå†…å­˜: {total_memory:.1f}GB")  # è¾“å‡ºæ˜¾å­˜æ€»é‡

        torch.cuda.empty_cache()  # å†æ¬¡æ¸…ç†ç¼“å­˜ï¼Œä¿è¯åç»­åŠ è½½å……è¶³æ˜¾å­˜
        logger.info("å·²æ¸…ç†GPUç¼“å­˜")  # è®°å½•æ“ä½œ

    estimated_model_size_gb = 0  # åˆå§‹åŒ–æ¨¡å‹å¤§å°ä¼°è®¡å€¼
    if hasattr(config, 'num_parameters'):  # è‹¥é…ç½®æ–‡ä»¶æä¾›å‚æ•°æ€»æ•°
        estimated_model_size_gb = config.num_parameters * 2 / 1024 ** 3  # æŒ‰fp16å‡è®¾æ¢ç®—ä¸ºGB (å‚æ•°é‡ *2 bytes / 1024^3)
    else:
        model_name_lower = model_args.model_name_or_path.lower()  # å°†æ¨¡å‹åè½¬å°å†™ï¼Œä¾¿äºåŒ¹é…
        if '70b' in model_name_lower or '72b' in model_name_lower:
            estimated_model_size_gb = 140  # 70Bç­‰çº§æ¨¡å‹çº¦140GB(fp16)
        elif '32b' in model_name_lower or '34b' in model_name_lower:
            estimated_model_size_gb = 64
        elif '13b' in model_name_lower or '14b' in model_name_lower:
            estimated_model_size_gb = 26
        elif '7b' in model_name_lower or '8b' in model_name_lower:
            estimated_model_size_gb = 14
        elif '3b' in model_name_lower:
            estimated_model_size_gb = 6
        else:
            estimated_model_size_gb = 10  # é»˜è®¤ç»™å‡ºä¿å®ˆä¼°è®¡

    logger.info(f"ä¼°ç®—æ¨¡å‹å¤§å°: {estimated_model_size_gb:.1f}GB")  # è¾“å‡ºä¼°ç®—ç»“æœ

    num_gpus = torch.cuda.device_count()  # å†æ¬¡è®°å½•GPUæ•°é‡ä¾›åç»­é€»è¾‘
    is_distributed = accelerator.num_processes > 1  # æ˜¯å¦å¤„äºå¤šè¿›ç¨‹è®­ç»ƒ

    if is_distributed:  # å¤šè¿›ç¨‹è®­ç»ƒä¸‹
        if script_args.use_tensor_parallel and estimated_model_size_gb > 20:  # è‹¥å…è®¸å¼ é‡å¹¶è¡Œä¸”æ¨¡å‹è¾ƒå¤§
            logger.info(f"ğŸ”§ ä½¿ç”¨å¼ é‡å¹¶è¡Œç­–ç•¥ (æ¨¡å‹å¤§å°: {estimated_model_size_gb:.1f}GB)")
            use_tensor_parallel = True  # é»˜è®¤å¯ç”¨å¼ é‡å¹¶è¡Œ

            import pkg_resources  # å»¶è¿Ÿå¯¼å…¥pkg_resourcesç”¨äºç‰ˆæœ¬æ¯”è¾ƒ
            torch_version = pkg_resources.get_distribution("torch").version  # è¯»å–å½“å‰PyTorchç‰ˆæœ¬
            if pkg_resources.parse_version(torch_version) < pkg_resources.parse_version("2.5.0"):  # ç‰ˆæœ¬ä¸è¶³
                logger.warning(f"âš ï¸ å½“å‰PyTorchç‰ˆæœ¬ {torch_version} ä¸æ”¯æŒå¼ é‡å¹¶è¡Œï¼Œéœ€è¦ >= 2.5.0")
                logger.warning("âš ï¸ è‡ªåŠ¨åˆ‡æ¢åˆ°DDPæ¨¡å¼")
                use_tensor_parallel = False  # å›é€€åˆ°DDP
            else:
                logger.info(f"âœ… PyTorchç‰ˆæœ¬ {torch_version} æ”¯æŒå¼ é‡å¹¶è¡Œ")
        else:
            logger.info(f"ğŸ”§ ä½¿ç”¨DDPè¿›è¡Œå¤šGPUè®­ç»ƒ (æ¨¡å‹å¤§å°: {estimated_model_size_gb:.1f}GB)")
            use_tensor_parallel = False  # é»˜è®¤ä½¿ç”¨ä¼ ç»ŸDDP
    else:
        logger.info("ğŸ”§ å•è¿›ç¨‹è®­ç»ƒ")  # å•æœºå•è¿›ç¨‹æ—¶
        use_tensor_parallel = True  # å…è®¸device_mapè‡ªåŠ¨åˆ‡åˆ†ï¼ˆä¹Ÿå¯èƒ½åªæ˜¯å•å¡ï¼‰

    model_kwargs = {  # å‡†å¤‡åŠ è½½æ¨¡å‹çš„å…³é”®å­—å‚æ•°
        "config": config,
        "torch_dtype": torch_dtype,
        "trust_remote_code": model_args.trust_remote_code,
        "quantization_config": quantization_config,
        "low_cpu_mem_usage": True,  # ä½¿ç”¨ä½CPUå†…å­˜åŠ è½½æ–¹å¼
    }

    if use_tensor_parallel:  # å½“ä½¿ç”¨device_mapåˆ‡åˆ†æ¨¡å‹
        model_kwargs["device_map"] = "auto"  # è‡ªåŠ¨æ ¹æ®æ˜¾å­˜åˆ‡åˆ†æ¨¡å—

        if num_gpus > 1:  # å¤šGPUæ—¶è®¾ç½®æ¯å¡æœ€å¤§æ˜¾å­˜é™åˆ¶
            max_memory = {}
            for i in range(num_gpus):
                gpu_props = torch.cuda.get_device_properties(i)  # è·å–æ˜¾å¡å±æ€§
                total_mem = gpu_props.total_memory  # æ€»æ˜¾å­˜ï¼ˆå­—èŠ‚ï¼‰
                usable_mem = int(total_mem * 0.8)  # é¢„ç•™20%ç¼“å†²
                max_memory[i] = f"{usable_mem // (1024 ** 3)}GiB"  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²è¡¨ç¤ºï¼Œä¾‹å¦‚"60GiB"

            model_kwargs["max_memory"] = max_memory  # ä¼ å…¥max_memoryå­—å…¸é™åˆ¶å ç”¨
            logger.info(f"ğŸ”§ å¼ é‡å¹¶è¡Œé…ç½®:")
            logger.info(f"  device_map: auto")
            logger.info(f"  max_memory: {max_memory}")
    else:
        logger.info("ğŸ”§ DDPé…ç½®: ä¸ä½¿ç”¨device_map")  # DDPæ¨¡å¼ç›´æ¥ç”±Accelerateè´Ÿè´£å‚æ•°åˆ†å‘

    try:  # å°è¯•åŠ è½½åŸºç¡€æ¨¡å‹
        model = AutoModelForCausalLM.from_pretrained(
            model_args.model_name_or_path,
            **model_kwargs
        )
        logger.info("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    except OSError as e:  # å¤„ç†device_mapä¸è¢«æ”¯æŒçš„æƒ…å†µ
        if "tensor parallel is only supported for" in str(e):  # ç‰¹å®šæŠ¥é”™æŒ‡ç¤ºå¼ é‡å¹¶è¡Œä¸è¢«æ”¯æŒ
            logger.error(f"âŒ å¼ é‡å¹¶è¡ŒåŠ è½½å¤±è´¥: {e}")
            logger.info("ğŸ”„ å°è¯•ä½¿ç”¨DDPæ¨¡å¼é‡æ–°åŠ è½½...")
            if "device_map" in model_kwargs:
                del model_kwargs["device_map"]  # ç§»é™¤device_mapçº¦æŸ
            if "max_memory" in model_kwargs:
                del model_kwargs["max_memory"]  # ç§»é™¤æ˜¾å­˜é™åˆ¶

            model = AutoModelForCausalLM.from_pretrained(  # å†æ¬¡åŠ è½½æ¨¡å‹
                model_args.model_name_or_path,
                **model_kwargs
            )
            logger.info("âœ… ä½¿ç”¨DDPæ¨¡å¼åŠ è½½æ¨¡å‹æˆåŠŸ")
        else:
            raise  # å…¶ä»–é”™è¯¯ç›´æ¥æŠ›å‡º

    logger.info("ğŸ“Š æ¨¡å‹åˆ†å¸ƒæƒ…å†µ:")  # è¾“å‡ºæ¨¡å‹è®¾å¤‡åˆ†å¸ƒ
    if hasattr(model, 'hf_device_map') and model.hf_device_map:  # å¦‚æœæ¨¡å‹è‡ªå¸¦device_mapè¯´æ˜å·²åˆ†ç‰‡
        logger.info("ğŸ”§ ä½¿ç”¨HuggingFaceè®¾å¤‡æ˜ å°„:")
        for module_name, device in model.hf_device_map.items():  # éå†æ¯ä¸ªæ¨¡å—æ‰€åœ¨è®¾å¤‡
            logger.info(f"  {module_name}: {device}")

        device_count = {}  # ç»Ÿè®¡æ¯ä¸ªè®¾å¤‡æ‰¿è½½æ¨¡å—æ•°é‡
        for device in model.hf_device_map.values():
            device_str = str(device)
            device_count[device_str] = device_count.get(device_str, 0) + 1

        logger.info("ğŸ“ˆ è®¾å¤‡ä½¿ç”¨ç»Ÿè®¡:")
        for device, count in device_count.items():
            logger.info(f"  {device}: {count} ä¸ªæ¨¡å—")
    else:
        device_params = {}  # å­—å…¸è·Ÿè¸ªå„è®¾å¤‡å‚æ•°æ•°é‡
        total_params = 0  # å…¨éƒ¨å‚æ•°æ•°é‡
        for name, param in model.named_parameters():  # éå†æ¯ä¸ªå¯è®­ç»ƒå‚æ•°
            device = str(param.device)
            if device not in device_params:
                device_params[device] = {'count': 0, 'size': 0}
            device_params[device]['count'] += 1
            device_params[device]['size'] += param.numel()  # ç´¯åŠ å‚æ•°å…ƒç´ æ•°
            total_params += param.numel()

        logger.info("ğŸ“ˆ å‚æ•°è®¾å¤‡åˆ†å¸ƒ:")
        for device, info in device_params.items():
            param_size_gb = info['size'] * 4 / 1024 ** 3  # å‡è®¾float32ï¼ˆ4å­—èŠ‚ï¼‰ä¼°ç®—å ç”¨
            percentage = info['size'] / total_params * 100  # è®¡ç®—ç™¾åˆ†æ¯”
            logger.info(f"  {device}: {info['count']} ä¸ªå‚æ•°ç»„, {param_size_gb:.2f}GB ({percentage:.1f}%)")

    if torch.cuda.is_available():  # é¢å¤–æ‰“å°å½“å‰æ˜¾å­˜å ç”¨
        logger.info("ğŸ’¾ GPUå†…å­˜ä½¿ç”¨æƒ…å†µ:")
        for i in range(torch.cuda.device_count()):
            allocated = torch.cuda.memory_allocated(i) / 1024 ** 3  # å·²åˆ†é…æ˜¾å­˜GB
            cached = torch.cuda.memory_reserved(i) / 1024 ** 3  # ç¼“å­˜æ˜¾å­˜GB
            total = torch.cuda.get_device_properties(i).total_memory / 1024 ** 3  # æ€»æ˜¾å­˜GB
            logger.info(f"  GPU {i}: å·²åˆ†é…={allocated:.1f}GB, ç¼“å­˜={cached:.1f}GB, æ€»è®¡={total:.1f}GB")

    if script_args.use_peft:  # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦æ³¨å…¥LoRA
        logger.info("ğŸ”§ é…ç½®LoRA")  # æ—¥å¿—æç¤ºè¿›å…¥LoRAè®¾ç½®

        if script_args.peft_path is not None:  # è‹¥æä¾›å·²æœ‰LoRAè·¯å¾„
            model = PeftModel.from_pretrained(model, script_args.peft_path, is_trainable=True)  # åŠ è½½LoRAæƒé‡å¹¶ä¿æŒå¯è®­ç»ƒ
        else:
            if model_args.load_in_8bit or model_args.load_in_4bit:  # å¦‚æœæ¨¡å‹ä¸ºä½æ¯”ç‰¹é‡åŒ–
                model = prepare_model_for_kbit_training(model, training_args.gradient_checkpointing)  # è°ƒæ•´å±‚çš„dtypeå’Œæ¢¯åº¦é…ç½®ä»¥é€‚åº”k-bitè®­ç»ƒ

            target_modules = script_args.target_modules.split(',') if script_args.target_modules else None  # è§£æç”¨æˆ·ä¼ å…¥çš„ç›®æ ‡æ¨¡å—
            if target_modules and 'all' in target_modules:  # allè¡¨ç¤ºè‡ªåŠ¨å‘ç°æ‰€æœ‰çº¿æ€§å±‚
                target_modules = find_all_linear_names(model, int4=model_args.load_in_4bit,
                                                       int8=model_args.load_in_8bit)

            modules_to_save = script_args.modules_to_save  # è¯»å–éœ€è¦å•ç‹¬ä¿å­˜çš„æ¨¡å—
            if modules_to_save is not None:
                modules_to_save = modules_to_save.split(',')  # è½¬ä¸ºåˆ—è¡¨

            peft_config = LoraConfig(  # æ„å»ºLoRAé…ç½®
                task_type=TaskType.CAUSAL_LM,
                target_modules=target_modules,
                inference_mode=False,
                r=script_args.lora_rank,
                lora_alpha=script_args.lora_alpha,
                lora_dropout=script_args.lora_dropout,
                modules_to_save=modules_to_save
            )
            model = get_peft_model(model, peft_config)  # å°†LoRAé€‚é…å™¨æ³¨å…¥æ¨¡å‹

        for param in filter(lambda p: p.requires_grad, model.parameters()):  # éå†æ‰€æœ‰éœ€è¦æ¢¯åº¦çš„å‚æ•°
            param.data = param.data.to(torch.float32)  # ä¿è¯LoRAæƒé‡ä½¿ç”¨fp32å­˜å‚¨ï¼Œæé«˜æ•°å€¼ç¨³å®šæ€§

        model.print_trainable_parameters()  # æ‰“å°LoRAåå¯è®­ç»ƒå‚æ•°ç»Ÿè®¡
    else:
        logger.info("ğŸ”§ å…¨å‚æ•°è®­ç»ƒæ¨¡å¼")  # ä¸ä½¿ç”¨LoRAæ—¶è¿›è¡Œå…¨é‡å¾®è°ƒ
        model = model.float()  # å°†æ¨¡å‹å…¨éƒ¨è½¬æ¢åˆ°float32ä»¥ç¨³å®šè®­ç»ƒ
        print_trainable_parameters(model)  # è¾“å‡ºå…¨å‚æ•°è®­ç»ƒçš„å‚æ•°è§„æ¨¡

    logger.info("ğŸ”„ å¼€å§‹åŠ è½½æ•°æ®é›†...")  # æ ‡è®°æ•°æ®åŠ è½½é˜¶æ®µ
    raw_datasets = load_datasets(data_args, model_args)  # æ ¹æ®å‚æ•°è§£æå‡ºçš„è·¯å¾„æˆ–Hubåç§°è¯»å–DatasetDict

    logger.info("ğŸ”„ å¼€å§‹é¢„å¤„ç†æ•°æ®é›†...")  # æç¤ºé¢„å¤„ç†å¼€å§‹
    preprocess_function = create_preprocess_function(tokenizer, prompt_template, script_args, IGNORE_INDEX)  # æ„é€ datasets.mapä½¿ç”¨çš„é¢„å¤„ç†å‡½æ•°

    train_dataset = None  # åˆå§‹åŒ–è®­ç»ƒé›†åˆ
    max_train_samples = 0  # è®°å½•å®é™…ä½¿ç”¨çš„è®­ç»ƒæ ·æœ¬æ•°
    if training_args.do_train:  # å¦‚æœéœ€è¦è®­ç»ƒ
        if "train" not in raw_datasets:  # æ£€æŸ¥train splitæ˜¯å¦å­˜åœ¨
            raise ValueError("--do_train requires a train dataset")  # ç¼ºå°‘è®­ç»ƒé›†æ—¶ç«‹å³æŠ¥é”™æç¤º
        train_dataset = raw_datasets['train'].shuffle(seed=42)  # æ‰“ä¹±è®­ç»ƒé›†ï¼Œè¿”å›æ–°çš„Datasetå¯¹è±¡
        max_train_samples = len(train_dataset)  # è®°å½•åˆå§‹æ ·æœ¬æ•°
        if data_args.max_train_samples is not None and data_args.max_train_samples > 0:  # å¦‚æœæŒ‡å®šäº†é‡‡æ ·ä¸Šé™
            max_train_samples = min(len(train_dataset), data_args.max_train_samples)  # æ›´æ–°æœ€å¤§æ ·æœ¬æ•°
            train_dataset = train_dataset.select(range(max_train_samples))  # é€‰æ‹©å‰Nä¸ªæ ·æœ¬ï¼Œè¿”å›æ–°çš„Dataset

        logger.debug(f"Example train_dataset[0]: {train_dataset[0]}")  # æ‰“å°ç¤ºä¾‹æ ·æœ¬ç»“æ„

        tokenized_dataset = train_dataset.map(  # è°ƒç”¨mapæ‰¹é‡å¤„ç†ï¼Œè¾“å‡ºå­—æ®µä¸ºinput_ids/attention_mask/labels
            preprocess_function,  # æŒ‡å®šåˆšæ‰æ„é€ çš„é¢„å¤„ç†å‡½æ•°
            batched=True,  # å¯ç”¨æ‰¹å¤„ç†æ¨¡å¼ï¼ŒåŠ é€Ÿtokenization
            num_proc=data_args.preprocessing_num_workers,  # å¹¶è¡Œworkeræ•°é‡ï¼Œæé«˜åå
            remove_columns=train_dataset.column_names,  # åˆ é™¤åŸå§‹åˆ—ï¼Œåªä¿ç•™æ–°ç”Ÿæˆå­—æ®µ
            load_from_cache_file=not data_args.overwrite_cache,  # é»˜è®¤ä½¿ç”¨ç¼“å­˜ä»¥é¿å…é‡å¤è®¡ç®—
            desc="Running tokenizer on dataset",  # è¿›åº¦æ¡æè¿°
        )
        train_dataset = tokenized_dataset.filter(  # è¿‡æ»¤æ‰ç©ºæ ‡ç­¾æ ·æœ¬
            lambda example: filter_empty_labels(example, IGNORE_INDEX),  # é’ˆå¯¹æ¯æ¡æ ·æœ¬æ‰§è¡Œè¿‡æ»¤é€»è¾‘
            num_proc=data_args.preprocessing_num_workers  # å¹¶è¡Œworkeræ•°é‡ä¸mapä¿æŒä¸€è‡´
        )

        logger.debug(f"Num train_samples: {len(train_dataset)}")  # è¾“å‡ºè¿‡æ»¤åæ ·æœ¬æ•°
        logger.debug("Tokenized training example:")  # æ‰“å°æç¤ºï¼Œä»¥ä¸‹å±•ç¤ºtokenåŒ–åçš„æ ·ä¾‹
        logger.debug(f"Decode input_ids[0]:\n{tokenizer.decode(train_dataset[0]['input_ids'])}")  # è§£ç input_idsæŸ¥çœ‹å†…å®¹
        replaced_labels = [label if label != IGNORE_INDEX else tokenizer.pad_token_id
                           for label in list(train_dataset[0]['labels'])]  # å°†IGNORE_INDEXæ›¿æ¢ä¸ºpad tokenä¾¿äºè§£ç é¢„è§ˆ
        logger.debug(f"Decode labels[0]:\n{tokenizer.decode(replaced_labels)}")

    eval_dataset = None  # åˆå§‹åŒ–éªŒè¯é›†åˆ
    max_eval_samples = 0  # è®°å½•éªŒè¯æ ·æœ¬æ•°é‡
    if training_args.do_eval:  # å¦‚æœéœ€è¦è¯„ä¼°
        if "validation" not in raw_datasets:
            raise ValueError("--do_eval requires a validation dataset")  # æ— éªŒè¯é›†æ—¶æŠ›å‡ºå¼‚å¸¸
        eval_dataset = raw_datasets["validation"]  # è¯»å–éªŒè¯split
        max_eval_samples = len(eval_dataset)
        if data_args.max_eval_samples is not None and data_args.max_eval_samples > 0:
            max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)
            eval_dataset = eval_dataset.select(range(max_eval_samples))
        eval_size = len(eval_dataset)
        logger.debug(f"Num eval_samples: {eval_size}")
        if eval_size > 500:
            logger.warning(f"Num eval_samples is large: {eval_size}, training slow, consider reduce it by `--max_eval_samples=50`")  # æé†’éªŒè¯é›†è¿‡å¤§å¯èƒ½æ‹–æ…¢è®­ç»ƒ
        logger.debug(f"Example eval_dataset[0]: {eval_dataset[0]}")  # æ‰“å°éªŒè¯é›†æ ·ä¾‹
        eval_dataset = eval_dataset.map(  # å¯¹éªŒè¯é›†æ‰§è¡Œç›¸åŒtokenåŒ–æ­¥éª¤
            preprocess_function,  # å¤ç”¨è®­ç»ƒé˜¶æ®µçš„é¢„å¤„ç†é€»è¾‘
            batched=True,  # æŒ‰æ‰¹æ¬¡é¢„å¤„ç†
            num_proc=data_args.preprocessing_num_workers,  # å¹¶è¡Œworkeræ•°é‡
            remove_columns=eval_dataset.column_names,  # ç§»é™¤åŸå§‹åˆ—
            load_from_cache_file=not data_args.overwrite_cache,  # é»˜è®¤ä½¿ç”¨ç¼“å­˜
            desc="Running tokenizer on validation dataset",  # è®¾å®šè¿›åº¦æ¡æ–‡å­—
        )
        eval_dataset = eval_dataset.filter(
            lambda example: filter_empty_labels(example, IGNORE_INDEX),  # åŒæ ·è¿‡æ»¤ç©ºæ ‡ç­¾æ ·æœ¬
            num_proc=data_args.preprocessing_num_workers  # å¯é€‰å¤šè¿›ç¨‹è¿‡æ»¤
        )
    logger.debug(f"Num eval_samples: {len(eval_dataset)}")  # è¾“å‡ºtokenåŒ–åéªŒè¯æ ·æœ¬æ•°é‡
    logger.debug("Tokenized eval example:")  # æç¤ºæ¥ä¸‹æ¥å±•ç¤ºtokenåŒ–åçš„éªŒè¯æ ·æœ¬
    logger.debug(tokenizer.decode(eval_dataset[0]['input_ids']))  # è§£ç æŸ¥çœ‹éªŒè¯æ ·æœ¬å†…å®¹

    logger.info("âœ… æ•°æ®é›†é¢„å¤„ç†å®Œæˆ")  # æ ‡è®°æ•°æ®å‡†å¤‡å®Œæ¯•

    data_collator = DataCollatorForSeq2Seq(  # åˆå§‹åŒ–æ•°æ®æ•´ç†å™¨
        tokenizer=tokenizer,  # æä¾›åˆ†è¯å™¨ä»¥æ‰§è¡Œpadding
        model=model,  # æä¾›æ¨¡å‹ä»¥ä¾¿collatorèƒ½è·å–ç‰¹æ®Šå‚æ•°ï¼ˆå¦‚prepare_decoder_input_idsï¼‰
        label_pad_token_id=IGNORE_INDEX,  # æ ‡ç­¾paddingå€¼ä½¿ç”¨IGNORE_INDEXï¼Œä¿è¯losså¿½ç•¥
        pad_to_multiple_of=4 if tokenizer.padding_side == "right" else None,  # è‹¥å³ä¾§paddingåˆ™æŒ‰4å¯¹é½ï¼Œåˆ©äºå¼ é‡æ ¸å±•å¼€
    )

    train_dataloader = None  # è®­ç»ƒæ•°æ®åŠ è½½å™¨é»˜è®¤ç©º
    eval_dataloader = None  # éªŒè¯æ•°æ®åŠ è½½å™¨é»˜è®¤ç©º

    if training_args.do_train and train_dataset is not None:  # å½“å­˜åœ¨è®­ç»ƒé›†
        train_dataloader = torch.utils.data.DataLoader(
            train_dataset,  # Datasetå¯¹è±¡ï¼Œå†…éƒ¨æŒ‰ç´¢å¼•è¿”å›æ ·æœ¬
            batch_size=training_args.per_device_train_batch_size,  # æ¯å¼ å¡çš„æ‰¹å¤§å°
            shuffle=True,  # æŒ‰epochéšæœºæ‰“ä¹±
            collate_fn=data_collator,  # ä½¿ç”¨ä¸Šé¢çš„collatorå®Œæˆpaddingå’Œå¼ é‡å †å 
        )  # DataLoaderè¾“å‡ºbatchå­—å…¸ï¼šinput_ids(batch, seq_len)ã€attention_maskåŒå½¢çŠ¶ã€labelsåŒå½¢çŠ¶

    if training_args.do_eval and eval_dataset is not None:  # å½“å­˜åœ¨éªŒè¯é›†
        eval_dataloader = torch.utils.data.DataLoader(
            eval_dataset,  # éªŒè¯æ•°æ®é›†
            batch_size=training_args.per_device_eval_batch_size,  # éªŒè¯é˜¶æ®µæ‰¹å¤§å°
            shuffle=False,  # éªŒè¯æ— éœ€æ‰“ä¹±
            collate_fn=data_collator,  # åŒä¸€collatorä¿è¯å½¢çŠ¶ä¸€è‡´
        )

    optimizer = None  # åˆå§‹åŒ–ä¼˜åŒ–å™¨
    lr_scheduler = None  # åˆå§‹åŒ–å­¦ä¹ ç‡è°ƒåº¦å™¨

    if training_args.do_train:  # ä»…åœ¨è®­ç»ƒæ¨¡å¼ä¸‹é…ç½®ä¼˜åŒ–å™¨
        optimizer = torch.optim.AdamW(  # ä½¿ç”¨AdamWä¼˜åŒ–LoRAæˆ–å…¨å‚æ•°æƒé‡
            filter(lambda p: p.requires_grad, model.parameters()),  # ä»…æ›´æ–°éœ€è¦æ¢¯åº¦çš„å‚æ•°ï¼ˆLoRAå±‚ï¼‰
            lr=training_args.learning_rate,  # åˆå§‹å­¦ä¹ ç‡
            weight_decay=training_args.weight_decay,  # L2æ­£åˆ™
        )

        num_update_steps_per_epoch = len(train_dataloader) // training_args.gradient_accumulation_steps  # æ¯ä¸ªepochä¸­çœŸå®çš„å‚æ•°æ›´æ–°æ­¥æ•°= (steps_per_epoch / ç´¯ç§¯æ­¥æ•°)
        max_train_steps = training_args.num_train_epochs * num_update_steps_per_epoch  # æ€»æ›´æ–°æ­¥æ•°=epochæ•°*æ¯epochæ›´æ–°æ¬¡æ•°

        lr_scheduler = get_linear_schedule_with_warmup(  # æ„é€ çº¿æ€§warmup+çº¿æ€§è¡°å‡å­¦ä¹ ç‡ç­–ç•¥
            optimizer=optimizer,  # ç»‘å®šä¼˜åŒ–å™¨
            num_warmup_steps=int(max_train_steps * training_args.warmup_ratio),  # warmupæ­¥æ•°=æ€»æ­¥æ•°*æ¯”ä¾‹
            num_training_steps=max_train_steps,  # è®¡åˆ’çš„å…¨éƒ¨è®­ç»ƒæ­¥æ•°
        )

    logger.info("ğŸ”„ å¼€å§‹å‡†å¤‡è®­ç»ƒç»„ä»¶...")  # è¿›å…¥Accelerateå‡†å¤‡é˜¶æ®µ

    model_is_distributed = hasattr(model, 'hf_device_map') and model.hf_device_map  # åˆ¤æ–­æ¨¡å‹æ˜¯å¦å·²æŒ‰device_mapåˆ‡åˆ†

    if model_is_distributed:  # å¦‚æœæ¨¡å‹å·²ç»é¢„åˆ†é…åˆ°å¤šä¸ªè®¾å¤‡
        logger.info("ğŸ”§ æ£€æµ‹åˆ°æ¨¡å‹å·²åˆ†å¸ƒåœ¨å¤šè®¾å¤‡ï¼Œä½¿ç”¨å…¼å®¹æ¨¡å¼")  # æ—¥å¿—æç¤ºå½“å‰é‡‡ç”¨å…¼å®¹å¤„ç†
        if training_args.do_train:  # å½“æ‰§è¡Œè®­ç»ƒæ—¶
            optimizer, train_dataloader, lr_scheduler = accelerator.prepare(  # ä»…å°†ä¼˜åŒ–å™¨å’Œdataloaderäº¤ç»™accelerator
                optimizer, train_dataloader, lr_scheduler
            )
            if eval_dataloader is not None:
                eval_dataloader = accelerator.prepare(eval_dataloader)
        else:
            if eval_dataloader is not None:
                eval_dataloader = accelerator.prepare(eval_dataloader)

        model.train() if training_args.do_train else model.eval()  # æ‰‹åŠ¨è®¾ç½®æ¨¡å‹mode

        logger.info("âœ… åˆ†å¸ƒå¼æ¨¡å‹è®­ç»ƒç»„ä»¶å‡†å¤‡å®Œæˆ")  # è®°å½•å…¼å®¹æ¨¡å¼ä¸‹å‡†å¤‡å®Œæ¯•
    else:
        logger.info("ğŸ”§ æ ‡å‡†æ¨¡å¼ï¼Œè®©Accelerateå¤„ç†æ‰€æœ‰ç»„ä»¶")  # æ—¥å¿—æç¤ºè¿›å…¥æ ‡å‡†æ¨¡å¼
        if training_args.do_train:
            model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(  # è®©AcceleratoråŒæ—¶åŒ…è£…æ¨¡å‹+ä¼˜åŒ–å™¨+æ•°æ®
                model, optimizer, train_dataloader, lr_scheduler
            )
            if eval_dataloader is not None:
                eval_dataloader = accelerator.prepare(eval_dataloader)
        else:
            model = accelerator.prepare(model)
            if eval_dataloader is not None:
                eval_dataloader = accelerator.prepare(eval_dataloader)

    logger.info("âœ… æ ‡å‡†è®­ç»ƒç»„ä»¶å‡†å¤‡å®Œæˆ")  # æ ‡è®°æ ‡å‡†æ¨¡å¼å‡†å¤‡å®Œæˆ

    if training_args.gradient_checkpointing and getattr(model, "supports_gradient_checkpointing", False):  # æ¡ä»¶å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Œå‡å°‘æ˜¾å­˜
        model.gradient_checkpointing_enable()
        if hasattr(model, "module"):
            model.module.config.use_cache = False  # å…³é—­KVç¼“å­˜ä»¥å…¼å®¹æ¢¯åº¦æ£€æŸ¥ç‚¹
            logger.info("Gradient checkpointing enabled for DDP model.")  # æ—¥å¿—è¯´æ˜DDPæ¨¡å‹å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
        else:
            model.config.use_cache = False
            logger.info("Gradient checkpointing enabled.")  # å•å¡æ¨¡å‹å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
    else:
        if hasattr(model, "module"):
            model.module.config.use_cache = True  # è‹¥æœªå¯ç”¨åˆ™æ¢å¤é»˜è®¤ç¼“å­˜
            logger.info("Gradient checkpointing disabled for DDP model.")  # æ—¥å¿—è¯´æ˜æœªå¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
        else:
            model.config.use_cache = True
            logger.info("Gradient checkpointing disabled.")  # å•å¡æ¨¡å‹ä¿æŒé»˜è®¤cacheè®¾ç½®
    if hasattr(model, "module"):
        model.module.enable_input_require_grads()  # ç¡®ä¿è¾“å…¥embeddingæ”¯æŒæ¢¯åº¦ï¼ˆLoRAå¸¸ç”¨ï¼‰
    else:
        model.enable_input_require_grads()

    logger.info("ğŸ‰ Accelerateå¤šGPUè®­ç»ƒé…ç½®æˆåŠŸï¼")

    if training_args.do_train:  # è®­ç»ƒä¸»å¾ªç¯å…¥å£
        logger.info("*** å¼€å§‹è®­ç»ƒ ***")

        model.train()  # åˆ‡æ¢åˆ°è®­ç»ƒæ¨¡å¼
        total_loss = 0  # ç´¯è®¡æŸå¤±ï¼Œç”¨äºæ—¥å¿—å¹³å‡
        completed_steps = 0  # å·²å®Œæˆçš„ä¼˜åŒ–æ­¥æ•°

        progress_bar = tqdm(  # åˆ›å»ºè¿›åº¦æ¡ï¼Œæ€»æ­¥æ•°=epochæ•°*æ¯epochè¿­ä»£æ•°
            range(int(training_args.num_train_epochs * len(train_dataloader))),  # è¿­ä»£ä¸Šé™åºåˆ—ï¼Œç”¨äºé©±åŠ¨è¿›åº¦æ˜¾ç¤º
            disable=not accelerator.is_local_main_process,  # éä¸»è¿›ç¨‹å…³é—­è¿›åº¦æ¡ï¼Œé¿å…å¤šå¡é‡å¤è¾“å‡º
            desc="Training"  # è¿›åº¦æ¡æ ‡é¢˜
        )

        for epoch in range(int(training_args.num_train_epochs)):  # å¤–å±‚å¾ªç¯éå†æ¯ä¸ªepoch
            logger.info(f"å¼€å§‹ç¬¬ {epoch + 1}/{int(training_args.num_train_epochs)} è½®è®­ç»ƒ")

            for step, batch in enumerate(train_dataloader):  # batchæ˜¯å­—å…¸ï¼šinput_ids(batch,seq)ã€attention_maskåŒshapeã€labelsåŒshape
                if model_is_distributed:  # å½“æ¨¡å‹å·²æ‰‹åŠ¨åˆ†å¸ƒæ—¶
                    outputs = model(**batch)  # å‰å‘ä¼ æ’­ï¼Œbatchå¼ é‡é»˜è®¤å·²åœ¨æ­£ç¡®è®¾å¤‡
                    loss = outputs.loss  # å–å‡ºå¹³å‡lossæ ‡é‡

                    if training_args.gradient_accumulation_steps > 1:
                        loss = loss / training_args.gradient_accumulation_steps  # ç´¯ç§¯æ¢¯åº¦æ—¶æŒ‰æ¯”ä¾‹ç¼©æ”¾loss

                    loss.backward()  # åå‘ä¼ æ’­ï¼Œè®¡ç®—æ¢¯åº¦

                    if (step + 1) % training_args.gradient_accumulation_steps == 0:  # è¾¾åˆ°ä¸€æ¬¡çœŸå®ä¼˜åŒ–æ­¥
                        if training_args.max_grad_norm > 0:
                            torch.nn.utils.clip_grad_norm_(model.parameters(), training_args.max_grad_norm)  # æ‰§è¡Œæ¢¯åº¦è£å‰ª

                        optimizer.step()  # æ›´æ–°æƒé‡
                        lr_scheduler.step()  # æ›´æ–°å­¦ä¹ ç‡
                        optimizer.zero_grad()  # æ¸…ç©ºç´¯ç§¯æ¢¯åº¦

                        completed_steps += 1  # è®°å½•å®Œæˆçš„ä¼˜åŒ–æ­¥æ•°
                        progress_bar.update(1)  # è¿›åº¦æ¡å‰è¿›ä¸€æ­¥
                else:
                    with accelerator.accumulate(model):  # Accelerateè‡ªåŠ¨å¤„ç†æ¢¯åº¦ç´¯ç§¯
                        outputs = model(**batch)
                        loss = outputs.loss

                        accelerator.backward(loss)  # ä½¿ç”¨AccelerateåŒ…è£…çš„åå‘ä¼ æ’­ï¼Œå…¼å®¹æ··åˆç²¾åº¦

                        if accelerator.sync_gradients:  # å½“è¾¾åˆ°ç´¯ç§¯æ­¥è§¦å‘åŒæ­¥æ—¶
                            accelerator.clip_grad_norm_(model.parameters(), training_args.max_grad_norm)

                        optimizer.step()  # åŒæ­¥æ—¶æ‰§è¡Œæƒé‡æ›´æ–°
                        lr_scheduler.step()  # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨
                        optimizer.zero_grad()  # æ¸…ç©ºæ¢¯åº¦ç¼“å­˜

                    if accelerator.sync_gradients:  # ä»…åœ¨çœŸå®æ›´æ–°æ­¥æ—¶å¢åŠ è®¡æ•°
                        completed_steps += 1  # å¢åŠ å®Œæˆæ­¥æ•°
                        progress_bar.update(1)  # æ›´æ–°è¿›åº¦æ¡

                total_loss += loss.detach().float()  # ç´¯åŠ å½“å‰lossï¼ˆdetaché˜²æ¢¯åº¦è¿é”ï¼‰

                step_completed = False  # æ ‡è®°æ˜¯å¦å®ŒæˆçœŸå®æ›´æ–°æ­¥
                if model_is_distributed:
                    step_completed = (step + 1) % training_args.gradient_accumulation_steps == 0  # å¼ é‡å¹¶è¡Œæƒ…å†µä¸‹çš„åˆ¤æ–­æ¡ä»¶
                else:
                    step_completed = accelerator.sync_gradients  # Accelerateæ¨¡å¼ä¸‹ç›´æ¥è¯»å–åŒæ­¥æ ‡å¿—

                if step_completed:  # ä»…åœ¨çœŸå®æ›´æ–°æ­¥æ‰§è¡Œæ—¥å¿—ã€ä¿å­˜ä¸è¯„ä¼°
                    if completed_steps % training_args.logging_steps == 0:  # æ—¥å¿—è®°å½•é—´éš”
                        avg_loss = total_loss / training_args.logging_steps  # è®¡ç®—è¿‡å»è‹¥å¹²æ­¥çš„å¹³å‡æŸå¤±
                        current_lr = lr_scheduler.get_last_lr()[0] if lr_scheduler else training_args.learning_rate  # å½“å‰å­¦ä¹ ç‡
                        logger.info(f"Step {completed_steps}: loss = {avg_loss:.4f}, lr = {current_lr:.2e}")
                        total_loss = 0  # é‡ç½®ç´¯è®¡loss

                    if training_args.save_steps > 0 and completed_steps % training_args.save_steps == 0:  # ä¿å­˜æ£€æŸ¥ç‚¹
                        output_dir = os.path.join(training_args.output_dir, f"checkpoint-{completed_steps}")  # ç»„åˆè¾“å‡ºè·¯å¾„
                        if model_is_distributed:
                            os.makedirs(output_dir, exist_ok=True)  # åˆ›å»ºç›®å½•ä»¥å­˜æ”¾åˆ†å¸ƒå¼æƒé‡
                            model.save_pretrained(output_dir)  # ä¿å­˜LoRA/å…¨å‚æ•°æƒé‡
                            tokenizer.save_pretrained(output_dir)
                            torch.save({  # ä¿å­˜ä¼˜åŒ–å™¨åŠLRè°ƒåº¦å™¨çŠ¶æ€ï¼Œä¾¿äºæ¢å¤è®­ç»ƒ
                                'optimizer': optimizer.state_dict(),
                                'lr_scheduler': lr_scheduler.state_dict() if lr_scheduler else None,
                                'completed_steps': completed_steps,
                            }, os.path.join(output_dir, 'training_state.pt'))
                        else:
                            accelerator.save_state(output_dir)  # Accelerateè‡ªåŠ¨ä¿å­˜æ¨¡å‹+ä¼˜åŒ–å™¨+éšæœºçŠ¶æ€
                        logger.info(f"ä¿å­˜æ£€æŸ¥ç‚¹åˆ°: {output_dir}")

                    if (training_args.do_eval and
                            training_args.eval_steps > 0 and
                            completed_steps % training_args.eval_steps == 0 and
                            eval_dataloader is not None):  # å‘¨æœŸæ€§è¯„ä¼°
                        model.eval()  # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
                        eval_loss = 0  # é‡ç½®è¯„ä¼°lossç´¯è®¡
                        eval_steps = 0  # é‡ç½®è¯„ä¼°æ­¥æ•°

                        for eval_batch in eval_dataloader:  # éå†è¯„ä¼°æ‰¹æ¬¡
                            with torch.no_grad():  # æ¨ç†é˜¶æ®µå…³é—­æ¢¯åº¦
                                eval_outputs = model(**eval_batch)  # å‰å‘æ¨ç†å¾—åˆ°loss
                                eval_loss += eval_outputs.loss.detach().float()  # ç´¯åŠ loss
                                eval_steps += 1  # å¢åŠ è¯„ä¼°æ­¥è®¡æ•°

                        avg_eval_loss = eval_loss / eval_steps  # è®¡ç®—å¹³å‡éªŒè¯æŸå¤±
                        try:
                            perplexity = math.exp(avg_eval_loss)  # å›°æƒ‘åº¦=exp(å¹³å‡loss)
                        except OverflowError:
                            perplexity = float("inf")  # Lossè¿‡å¤§å¯¼è‡´expæº¢å‡ºæ—¶è¿”å›æ— ç©·

                        logger.info(
                            f"Step {completed_steps}: eval_loss = {avg_eval_loss:.4f}, perplexity = {perplexity:.2f}")
                        model.train()  # è¯„ä¼°ååˆ‡å›è®­ç»ƒæ¨¡å¼
        progress_bar.close()  # ç»“æŸè¿›åº¦æ¡

        if accelerator.is_main_process:
            logger.info(f"ä¿å­˜æ¨¡å‹åˆ°: {training_args.output_dir}")  # ä»…ä¸»è¿›ç¨‹æ‰“å°ä¿å­˜ä¿¡æ¯

        unwrapped = get_unwrapped_model(model)  # æå–åŸå§‹æ¨¡å‹å¯¹è±¡
        unwrapped.config.use_cache = True  # æ¢å¤use_cacheè®¾ç½®ï¼Œä¾¿äºæ¨ç†
        unwrapped.enable_input_require_grads()  # æ¢å¤è¾“å…¥æ¢¯åº¦éœ€æ±‚ï¼ˆå¯¹æ¨ç†æ— å®³ï¼‰

        if model_is_distributed:
            logger.info("ğŸ”§ ä¿å­˜åˆ†å¸ƒå¼æ¨¡å‹...")  # è®°å½•å³å°†ä¿å­˜å·²ç»åˆ†ç‰‡çš„æ¨¡å‹
            model.save_pretrained(training_args.output_dir)  # ä¿å­˜åˆ†å¸ƒå¼æƒé‡ï¼ˆå«LoRAï¼‰
            tokenizer.save_pretrained(training_args.output_dir)  # ä¿å­˜åˆ†è¯å™¨
        else:
            accelerator.wait_for_everyone()  # åŒæ­¥æ‰€æœ‰è¿›ç¨‹

            if accelerator.is_main_process:
                unwrapped_model = accelerator.unwrap_model(model)  # å»é™¤AccelerateåŒ…è£…
                save_model(unwrapped_model, tokenizer, training_args.output_dir)  # è°ƒç”¨ç»Ÿä¸€ä¿å­˜å‡½æ•°
                logger.info("âœ… æ ‡å‡†æ¨¡å‹ä¿å­˜å®Œæˆ")

    if training_args.do_eval and eval_dataloader is not None:  # ç»ˆå±€è¯„ä¼°
        logger.info("*** æœ€ç»ˆè¯„ä¼° ***")  # æ—¥å¿—æç¤ºè¿›å…¥æœ€ç»ˆè¯„ä¼°é˜¶æ®µ
        model.eval()  # åˆ‡æ¢è¯„ä¼°æ¨¡å¼
        eval_loss = 0  # åˆå§‹åŒ–lossç´¯è®¡
        eval_steps = 0  # åˆå§‹åŒ–æ­¥è®¡æ•°

        for eval_batch in eval_dataloader:  # éå†å…¨éƒ¨éªŒè¯æ‰¹æ¬¡
            with torch.no_grad():  # ç¦æ­¢æ¢¯åº¦è®¡ç®—
                eval_outputs = model(**eval_batch)  # æ‰§è¡Œå‰å‘æ¨ç†
                eval_loss += eval_outputs.loss.detach().float()  # ç´¯åŠ loss
                eval_steps += 1  # å¢åŠ è¯„ä¼°æ­¥è®¡æ•°

        avg_eval_loss = eval_loss / eval_steps  # æ±‚å¹³å‡éªŒè¯æŸå¤±
        try:
            perplexity = math.exp(avg_eval_loss)  # è®¡ç®—æœ€ç»ˆå›°æƒ‘åº¦
        except OverflowError:
            perplexity = float("inf")  # é¿å…æŒ‡æ•°æº¢å‡º
        if accelerator.is_main_process:
            logger.info(f"æœ€ç»ˆè¯„ä¼°ç»“æœ: eval_loss = {avg_eval_loss:.4f}, perplexity = {perplexity:.2f}")  # ä¸»è¿›ç¨‹è¾“å‡ºæœ€ç»ˆæŒ‡æ ‡


if __name__ == "__main__":  # å½“è„šæœ¬ç›´æ¥æ‰§è¡Œæ—¶
    main()  # è¿è¡Œä¸»æµç¨‹
